# `frontend/public/robots.txt`

This file instructs web crawlers on how to index the site.

## Configuration

- **`User-agent: *`**: This directive specifies that the rules apply to all web crawlers (e.g., Googlebot, Bingbot).
- **`Disallow:`**: An empty `Disallow` directive means that no part of the site is disallowed.

## Behavior

This configuration permits all web crawlers to access and index the entire frontend application. This ensures that the single-page application is fully discoverable by search engines.